{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YrBeM9Mq0JCK",
        "XgrzaUwl_qSY"
      ],
      "authorship_tag": "ABX9TyOj75L1MYNLkoQI5AUPAglO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vieiramesquita/TheroPoDa/blob/main/TheroPoDa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Name\n",
        "T(h)eroPoDa - Time Series Extraction for Polygonal Data\n",
        "\n",
        "### Description\n",
        "Toolkit created to extract time series information from Sentinel 2 data stored in Earth Engine\n",
        "\n",
        "### Author\n",
        "  Vin√≠cius Vieira Mesquita - vieiramesquita@gmail.com\n",
        "\n",
        "### Version\n",
        "  1.0.5"
      ],
      "metadata": {
        "id": "8ZN0zzrtx5BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import main libraries\n",
        "\n",
        "Run the following cell to import the main API's into your session."
      ],
      "metadata": {
        "id": "MMa2J7N_G-ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install earthengine-api\n",
        "!pip install pandas\n",
        "!pip instal joblib"
      ],
      "metadata": {
        "id": "yf4RyPYxWb0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import ee\n",
        "import pandas as pd\n",
        "from joblib import Parallel, delayed"
      ],
      "metadata": {
        "id": "j332acITGpEz"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authenticate and initialize\n",
        "\n",
        "Run the `ee.Authenticate` function to authenticate your access to Earth Engine servers and `ee.Initialize` to initialize it. Upon running the following cell you'll be asked to grant Earth Engine access to your Google account. Follow the instructions printed to the cell."
      ],
      "metadata": {
        "id": "4Dr-6br_M9tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trigger the authentication flow.\n",
        "ee.Authenticate()\n",
        "\n",
        "# Initialize the library.\n",
        "ee.Initialize()\n"
      ],
      "metadata": {
        "id": "RX26uO4nM7s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the NDVI Time Series from Earth Engine \n",
        "\n",
        "Function responsible to get the time series of Sentinel 2 data throught Earth Engine.\n",
        "\n",
        "This function needs a `geometry` object in the `ee.Feature()` formart and the choosed vetor propertie ID as the `id_field`.\n",
        "\n"
      ],
      "metadata": {
        "id": "29j9yGY3SPr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Returns a NDVI time series (and other informations) by a target polygon\n",
        "def getTimeSeries(geometry,id_field,bestEffort=False):\n",
        "  \n",
        "  #Creates a Cloud and Shadow mask for the input Sentinel 2 image\n",
        "  def mask_and_ndvi(img):\n",
        "\n",
        "    #Get spacecraft plataform name\n",
        "    satName = ee.String(img.get('SPACECRAFT_NAME')) \n",
        "\n",
        "    #Mask expression for Sentinel 2 Surface Reflectance data\n",
        "    maskExp = \"(b('MSK_CLDPRB') < 5 && b('SCL') != 3 && b('SCL') != 10)\" \n",
        "\n",
        "    #Mask expression for Sentinel 2 Top of Atmosphere Data\n",
        "    #maskExp = \"b('Q60') == 0\" \n",
        "\n",
        "    #Remove cloud and shadow from images\n",
        "    mask = img.expression(maskExp).add(img.lte(0));\n",
        "\n",
        "    #Calculate NDVI (Normalized Difference Vegetation Index) based on Bands 4 (Red) and 8 (Near Infrared)\n",
        "    ndvi = img.updateMask(mask).normalizedDifference(['B8','B4']).select([0],['NDVI'])\n",
        "  \n",
        "    return (img.addBands(ndvi, None, True)\n",
        "      .set({'system:time_start':img.get('system:time_start'),'satelite':satName}))\n",
        "\n",
        "  #Extracts and standardizes the output NDVI values and etc. by each image\n",
        "  def reduceData(img):\n",
        "\n",
        "    img = ee.Image(img)\n",
        "\n",
        "    #Get the date which the image was taken\n",
        "    imgDate = ee.Date(ee.Number(img.get('system:time_start')))\n",
        "\n",
        "    #Organize the time for the outuput NDVI information\n",
        "    orgDate = (ee.String(ee.Number(imgDate.get('year')).toInt().format())\n",
        "      .cat('-')\n",
        "      .cat(ee.String(ee.Number(imgDate.get('month')).toInt().format()))\n",
        "      .cat('-')\n",
        "      .cat(ee.String(ee.Number(imgDate.get('day')).toInt().format()))\n",
        "      )\n",
        "    \n",
        "    #Defines the zonal reducers to use\n",
        "    reducers = (ee.Reducer.mean()\n",
        "        .combine(**{'reducer2': ee.Reducer.stdDev(),'sharedInputs':True,})\n",
        "        .combine(**{'reducer2': ee.Reducer.count(),'sharedInputs':True}))\n",
        "\n",
        "    #If polygon area is to big and causes memory limit error, bestEffort is used\n",
        "    #bestEffort - If the polygon would contain too many pixels at the given scale, compute and use a larger scale which would allow the operation to succeed.\n",
        "    if bestEffort == False: \n",
        "      series = img.reduceRegion(reducers,ee.Feature(geometry).geometry(), 10,None,None,False,1e13,16)\n",
        "    else:\n",
        "      series = img.reduceRegion(reducers,ee.Feature(geometry).geometry(), None,None,None,True,1e13,16)\n",
        "    \n",
        "    #Return defined information for the choosed polygon\n",
        "    return (ee.Feature(geometry)\n",
        "      .set('id',ee.String(img.id())) #Image ID\n",
        "      .set('date',orgDate) #Date\n",
        "      .set('satelite',img.get('satelite')) #Sapacraft plataform name (i.e. Sentinel 2A or 2B)\n",
        "      .set('MGRS_TILE',img.get('MGRS_TILE')) #Reference tile grid      \n",
        "      .set('AREA_HA',ee.Feature(geometry).area(1).divide(10000)) #Choosed polygon ID Field\n",
        "      .set('NDVI_mean',ee.Number(ee.Dictionary(series).get('NDVI_mean'))) #NDVI pixel average for the polygon\n",
        "      .set('NDVI_stdDev',ee.Number(ee.Dictionary(series).get('NDVI_stdDev'))) #NDVI pixel Standard Deviation for the polygon\n",
        "      .set('Pixel_Count',ee.Number(ee.Dictionary(series).get('NDVI_count'))) #Number of pixels cloudless and shadowless used for estimatives\n",
        "    )\n",
        "\n",
        "  #Turns Feature into Dictionary to get properties\n",
        "  def toDict(feat):\n",
        "    return ee.Feature(feat).toDictionary()\n",
        "\n",
        "  #imgCol =  (ee.ImageCollection(\"COPERNICUS/S2_HARMONIZED\")\n",
        "  #  .filterBounds(geometry.geometry())\n",
        "  #  .map(mask_img))\n",
        "\n",
        "  #Calls the Sentinel 2 data collection, filter the images based in the polygon location, masks cloud/shadow and calculates NDVI\n",
        "  imgCol = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
        "    .filterBounds(geometry.geometry())\n",
        "    .map(mask_and_ndvi))\n",
        "\n",
        "  #Extracts NDVI time series by polygon, remove the nulls and build a dictionary struture to the data\n",
        "  Coll_fill = (imgCol.toList(imgCol.size()).map(reduceData)\n",
        "    .filter(ee.Filter.notNull(['NDVI_mean']))\n",
        "    .map(toDict)\n",
        "    )\n",
        "    \n",
        "  return Coll_fill"
      ],
      "metadata": {
        "id": "-xI_LefsSPgg"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build and Structure the Time Series library\n",
        "\n",
        "Function responsible to build and structure the time series library."
      ],
      "metadata": {
        "id": "YrBeM9Mq0JCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Builds and writes a NDVI time series with Sentinel 2 data by a target vector asset\n",
        "def build_time_series(index,obj,id_field,outfile,asset,bestEffort=False):\n",
        "  \n",
        "  #Main polygon asset\n",
        "  samples = ee.FeatureCollection(asset).select(id_field)\n",
        "\n",
        "  #Creates an empty data.frame for the time series\n",
        "  df = pd.DataFrame()\n",
        "\n",
        "  #Processing time start variable\n",
        "  start_time_obj = time.time()\n",
        "\n",
        "  #Selects the target polygon\n",
        "  selected_sample = samples.filter(ee.Filter.eq(id_field,obj)).first()\n",
        "\n",
        "  #Extracts the formated NDVI time series from the target polygon\n",
        "  point_series = getTimeSeries(ee.Feature(selected_sample),id_field,bestEffort).getInfo()\n",
        "  \n",
        "  #Writes the time series by data frame row\n",
        "  for item in point_series:\n",
        "    df  = pd.concat([df,pd.DataFrame(item,index=[0])])\n",
        "  \n",
        "  #If the output file exists, ignores the header\n",
        "  hdr = False if os.path.isfile(outfile) else True\n",
        "  \n",
        "  #Rounds the NDVI values by four decimals (avoid huge and slow tables)\n",
        "  df['AREA_HA'] = df['AREA_HA'].round(decimals=4) \n",
        "  df['NDVI_mean'] = df['NDVI_mean'].round(decimals=4) \n",
        "  df['NDVI_stdDev'] = df['NDVI_stdDev'].round(decimals=4) \n",
        "\n",
        "  #Writes the time series in a table in appending mode ('a')\n",
        "  df.to_csv(outfile,mode='a',header = hdr,index = False,sep =';')\n",
        "\n",
        "  #Estimates the total time spent in the generation of the time series for the target polygon\n",
        "  time_spent = round(time.time() - start_time_obj, 3)\n",
        "\n",
        "  print(f'Index {index} - Object [{obj}] procesed in {round(time.time() - start_time_obj, 3)} seconds')\n",
        "\n",
        "  #Returns checkers\n",
        "  if df.shape[0] > 0: \n",
        "    return True,time_spent #if everthings works fine, returns the True and the time spend\n",
        "  elif float(df['AREA_HA']) < 0.01:\n",
        "    return None,None #If the polygon area is too small, ignores the polygon!\n",
        "  else:\n",
        "    return False,None #if something goes wrong, returns False"
      ],
      "metadata": {
        "id": "1n_GSXfk0IvV"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the Time Series library\n",
        "\n",
        "Function responsible to check the consistency of the time series library."
      ],
      "metadata": {
        "id": "XgrzaUwl_qSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checks if time series processing works\n",
        "def build_time_series_check(index,obj,id_field,outfile,asset,checker=False):\n",
        "\n",
        "  obj = int(obj)\n",
        "\n",
        "  #Checks if the polygon was been processed before\n",
        "  if checker is True:\n",
        "\n",
        "    df_check = pd.read_csv(outfile,sep =';')\n",
        "\n",
        "    if obj in list(df_check[id_field].values):\n",
        "\n",
        "      print(f' Object [{obj}] was found in the file. Skipping..')\n",
        "      return {'errors':None, 'time': 0}\n",
        "\n",
        "  errors = None\n",
        "  time = None\n",
        "\n",
        "  \n",
        "  try:\n",
        "    check = build_time_series(index,obj,id_field,outfile,asset)\n",
        "    time = check[1]\n",
        "\n",
        "    if check[0] == False:\n",
        "      print('raised')\n",
        "      raise\n",
        "    if check[0] == None:\n",
        "      return {'errors':'ignore' ,'time': 'ignore'}\n",
        "      \n",
        "  except:\n",
        "\n",
        "    try:\n",
        "\n",
        "      print(f'Index {index} - Request [{obj}] fails. Trying the best effort!')\n",
        "\n",
        "      check = build_time_series(index,obj,id_field,outfile,asset,True)\n",
        "\n",
        "      if check[0] == False:\n",
        "        print('raised')\n",
        "        raise\n",
        "      \n",
        "      if check[0] == None:\n",
        "        return {'errors':'ignore' ,'time': 'ignore'}\n",
        "      \n",
        "    except:\n",
        "\n",
        "      print(f'Index {index} - Request [{obj}] expired. Sending it to the error list!')\n",
        "        \n",
        "      errors = obj\n",
        "\n",
        "  return {'errors':errors ,'time': time}"
      ],
      "metadata": {
        "id": "SpNM9wFuAD1f"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the Polygon List file\n",
        "\n",
        "Function responsible to write a text file contaning each Polygon ID used to extract the time series."
      ],
      "metadata": {
        "id": "KOazO2JI-X3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Builds and writes the Polygon ID list\n",
        "def build_id_list(asset,id_field,colab_folder):\n",
        "  \n",
        "  #Loads EE Polygon asset\n",
        "  samples = ee.FeatureCollection(asset).select(id_field)\n",
        "  \n",
        "  #Estimates the number of polygons in the Asset\n",
        "  sample_size = int(samples.size().getInfo())\n",
        "\n",
        "  #Conditionals to avoid Earth Engine memory erros\n",
        "  #Earth Engine is limited to request 50k vectors, make manual lists if you need more!\n",
        "  if sample_size > 50000: \n",
        "    samples_list = samples.toList(50000)\n",
        "  else:\n",
        "    samples_list = samples.toList(samples.size())\n",
        "\n",
        "  fileName = os.path.join(colab_folder,'polygonList.txt')\n",
        "\n",
        "  with open(fileName, \"w\") as polygon_file:\n",
        "\n",
        "    def get_ids(feat):\n",
        "      return ee.Feature(feat).get(id_field)\n",
        "\n",
        "    samples_list_slice = samples_list.map(get_ids).sort().getInfo()\n",
        "\n",
        "    for polygon in samples_list_slice:\n",
        "      polygon_file.write(str(polygon)+ '\\n')\n",
        "  "
      ],
      "metadata": {
        "id": "WSpenzzJjbcO"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run\n",
        "\n",
        "Function responsible to catch argument information and start run the process."
      ],
      "metadata": {
        "id": "C7pp7bb4A40W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run(asset,id_field,output_name,colab_folder):\n",
        "\n",
        "  output_name = os.path.join(colab_folder,output_name)\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  fileName_polyList = os.path.join(colab_folder,'polygonList.txt')\n",
        "\n",
        "  #Reading the file which contains the polygons IDs\n",
        "  listPolygons_text = open(fileName_polyList,\"r\")\n",
        "  listPolygons_text = listPolygons_text.readlines()\n",
        "\n",
        "  #Format the data\n",
        "  listPolygons_text = [int(name) for name in listPolygons_text]\n",
        "\n",
        "  start_obj = 1\n",
        "\n",
        "  #Estimates the total of polygons\n",
        "  total = len(listPolygons_text)\n",
        "\n",
        "  print(f'Number of objects to process: {total}')\n",
        "\n",
        "  #Yes, it will take a long time to finish!\n",
        "  if total > 1000:\n",
        "    print('Go take a coffee and watch a series... it will take a while!')\n",
        "  \n",
        "  list_num = listPolygons_text[start_obj:total]\n",
        "\n",
        "  #Checkers\n",
        "  first_dict = [{'errors':'ignore' ,'time': 'ignore'}]\n",
        "  check_file = True\n",
        "\n",
        "  #Checks if the output file exists\n",
        "  if os.path.exists(output_name) is False:\n",
        "\n",
        "    #If false, creates one using the first polygon\n",
        "    check_file = False\n",
        "    print('Creating the main file..')\n",
        "    first_dict = build_time_series_check(0,int(listPolygons_text[0]),id_field,output_name,asset)\n",
        "\n",
        "  #Structures the arguments for jobLib::Parallel\n",
        "  worker_args = [\n",
        "    (listPolygons_text.index(obj),obj,id_field,output_name,asset,check_file) \\\n",
        "    for obj in list_num\n",
        "  ]\n",
        "\n",
        "  #Number of to use (more than 20 generate many sleeping queries)\n",
        "  n_cores = 16 #Recommended\n",
        "  \n",
        "  #Starts the parallel processing\n",
        "  infos = Parallel(n_jobs=n_cores, backend='multiprocessing')(delayed(build_time_series_check)(*args) for args in worker_args)\n",
        "\n",
        "  if check_file is True:\n",
        "    first_dict = {'time': 0}\n",
        "\n",
        "  #List with all times computed during the processing\n",
        "  time_list = [first_dict['time']] + [item['time'] for item in infos if item['time'] != None]\n",
        "\n",
        "  #List of polygons probably with errors\n",
        "  errors_list = [item['errors'] for item in infos if item['errors'] != None]\n",
        "\n",
        "  fileName_errors = os.path.join(colab_folder,'errors_polygon.txt')\n",
        "\n",
        "  #Write a file with the erros list\n",
        "  with open(fileName_errors, \"w\") as errors_file:  \n",
        "    for polygon in errors_list:\n",
        "      errors_file.write(str(polygon)+ '\\n')\n",
        "\n",
        "  print(f'The average processing time was {round(pd.DataFrame(time_list).mean()[0],2)} seconds')\n",
        "  print(f'Processing finished. All the work took {round(time.time() - start_time,3)} seconds to complete')"
      ],
      "metadata": {
        "id": "ZjWWmlIgDvwb"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  asset = 'users/vieiramesquita/LAPIG_FieldSamples/lapig_goias_fieldwork_2022_50m' #Earth Engine Vector Asset\n",
        "  id_field = 'ID_POINTS' #Vector collumn used as ID (use unique identifiers!)\n",
        "  colab_folder = '/content'\n",
        "  output_name = 'LAPIG_Pasture_S2_NDVI_Monitoring_FieldWork.csv' #Output filename\n",
        "  \n",
        "  #Check if polygon list file exists\n",
        "  if os.path.exists(os.path.join(colab_folder,'polygonList.txt')) is False:\n",
        "    build_id_list(asset,id_field,colab_folder)\n",
        "\n",
        "  #Let the party begin!\n",
        "  run(asset,id_field,output_name,colab_folder)"
      ],
      "metadata": {
        "id": "P_sbLkobD5Of"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}